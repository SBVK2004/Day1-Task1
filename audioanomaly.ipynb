import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns

from glob import glob

import librosa
import librosa.display
import IPython.display as ipd

from itertools import cycle

sns.set_theme(style="white", palette=None)
color_pal = plt.rcParams["axes.prop_cycle"].by_key()["color"]
color_cycle = cycle(plt.rcParams["axes.prop_cycle"].by_key()["color"])audio_files = glob('../input/ravdess-emotional-speech-audio/*/*.wav')
ipd.Audio(audio_files[0])
y, sr = librosa.load(audio_files[0])
print(f'y: {y[:10]}')
print(f'shape y: {y.shape}')
print(f'sr: {sr}')
y_trimmed, _ = librosa.effects.trim(y, top_db=20)
pd.Series(y_trimmed).plot(figsize=(10, 5),
                  lw=1,
                  title='Raw Audio Trimmed Example',
                 color=color_pal[1])
plt.show()
pd.Series(y[30000:30500]).plot(figsize=(10, 5),
                  lw=1,
                  title='Raw Audio Zoomed In Example',
                 color=color_pal[2])
plt.show()
D = librosa.stft(y)
S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)
S_db.shape
fig, ax = plt.subplots(figsize=(10, 5))
img = librosa.display.specshow(S_db,
                              x_axis='time',
                              y_axis='log',
                              ax=ax)
ax.set_title('Spectogram Example', fontsize=20)
fig.colorbar(img, ax=ax, format=f'%0.2f')
plt.show()
S = librosa.feature.melspectrogram(y=y,
                                   sr=sr,
                                   n_mels=128 * 2,)
S_db_mel = librosa.amplitude_to_db(S, ref=np.max)
fig, ax = plt.subplots(figsize=(10, 5))
# Plot the mel spectogram
img = librosa.display.specshow(S_db_mel,
                              x_axis='time',
                              y_axis='log',
                              ax=ax)
ax.set_title('Mel Spectogram Example', fontsize=20)
fig.colorbar(img, ax=ax, format=f'%0.2f')
plt.show()





#audio anomalies
pip install librosa scikit-learn matplotlib seaborn pandas
import os
import numpy as np
import pandas as pd
import librosa
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from glob import glob
import os

all_files = glob('../input/ravdess-emotional-speech-audio/*/*.wav')

normal_files = []
anomalous_files = []

for file in all_files:
    filename = os.path.basename(file)
    emotion_code = int(filename.split("-")[2])  # 3rd part tells emotion
    if emotion_code in [1, 2]:  # neutral or calm → normal
        normal_files.append(file)
    else:  # angry, fearful, etc. → anomalous
        anomalous_files.append(file)

print(f"Normal files: {len(normal_files)}")
print(f"Anomalous files: {len(anomalous_files)}")
def extract_mfcc_features(file_path, n_mfcc=13):
    y, sr = librosa.load(file_path, duration=5)  # Load 5 seconds
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)
    return np.mean(mfcc.T, axis=0)  # Average across time
X = []
y = []

# Label 0 = normal, 1 = anomalous
for file in normal_files:
    X.append(extract_mfcc_features(file))
    y.append(0)

for file in anomalous_files:
    X.append(extract_mfcc_features(file))
    y.append(1)

X = np.array(X)
y = np.array(y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
import librosa.display
import matplotlib.pyplot as plt

def show_audio_prediction(file_path):
    print("Prediction:", predict_audio(file_path))
    y, sr = librosa.load(file_path)
    plt.figure(figsize=(10, 3))
    librosa.display.waveshow(y, sr=sr)
    plt.title("Waveform of Test Audio")
    plt.show()

# Try it
show_audio_prediction("../input/ravdess-emotional-speech-audio/Actor_01/03-01-05-01-01-01-01.wav")

def predict_audio(file_path):
    features = extract_mfcc_features(file_path)
    result = model.predict([features])[0]
    return result  # 0 = normal, 1 = anomaly

def detect_and_respond(file_path):
    result = predict_audio(file_path)
    
    # Show wave and prediction
    y, sr = librosa.load(file_path)
    plt.figure(figsize=(10, 3))
    librosa.display.waveshow(y, sr=sr)
    plt.title("Waveform of Test Audio")
    plt.show()
    
    if result == 1:
        print(" Anomaly Detected in the Audio!")
        return ipd.Audio(file_path)  # Will play the anomalous sound
    else:
        print("Normal Audio - No Anomaly Detected.")
        return ipd.Audio(file_path)  # Optional to still play it

detect_and_respond("../input/ravdess-emotional-speech-audio/Actor_01/03-01-06-01-01-01-01.wav")

def extract_mfcc_features(file_path, n_mfcc=13):
    y, sr = librosa.load(file_path, duration=5)
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)
    return np.mean(mfcc.T, axis=0)

def predict_audio(file_path):
    features = extract_mfcc_features(file_path)
    result = model.predict([features])[0]
    return result

def detect_and_respond(file_path):
    result = predict_audio(file_path)
    if result == 1:
        print(" Anomaly Detected in the Audio!")
    else:
        print(" Normal Sound - No Anomaly Detected.")
    
    # Show waveform and play audio
    y, sr = librosa.load(file_path)
    plt.figure(figsize=(10, 3))
    librosa.display.waveshow(y, sr=sr)
    plt.title("Waveform")
    plt.show()
    
    return ipd.Audio(file_path)

detect_and_respond("../input/ravdess-emotional-speech-audio/Actor_02/03-01-06-01-01-01-02.wav")
